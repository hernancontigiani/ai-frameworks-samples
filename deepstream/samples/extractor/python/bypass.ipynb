{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bypass.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESuIobvoy0NX"
      },
      "source": [
        "# torch to ONNX"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAk7k8EKz2_F"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def to_onnx(model, input_shape, dynamic_shape=True, onnx_path=\"model.onnx\"):\n",
        "    model.eval()\n",
        "    dummy_input = torch.ones(*input_shape, dtype=torch.float32, device=cuda0)\n",
        "\n",
        "    input_names=['input']\n",
        "    output_names=['output']\n",
        "    if dynamic_shape:\n",
        "        dynamic_axes= {'input': {0:'batch_size'}, 'output': {0:'batch_size'}} # define batch_size as dynamic input\n",
        "    else:\n",
        "        dynamic_axes = None\n",
        "    \n",
        "    torch.onnx.export(model, dummy_input, onnx_path, input_names=input_names, output_names=output_names, dynamic_axes=dynamic_axes)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e52oLUHeKL74"
      },
      "source": [
        "### Bypass model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEC_Sdz-KUhf",
        "outputId": "44064eb2-ed2c-486e-ef5c-1e68691aef30"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "cuda0 = torch.device('cuda:0')\n",
        "\n",
        "batch_size = 1\n",
        "height = 256\n",
        "width = 128\n",
        "\n",
        "input_shape = (batch_size, 3, height, width)\n",
        "\n",
        "class BypassNet(torch.nn.Module):\n",
        "    def __init__(self, num_clasess=512):\n",
        "        super().__init__()\n",
        "        self.num_clasess = torch.tensor(num_clasess, device=cuda0)\n",
        "        vector = np.zeros(shape=num_clasess, dtype=np.float32)\n",
        "        self.vector = torch.from_numpy(vector).to(cuda0)\n",
        "    def forward(self, input):\n",
        "        \n",
        "\n",
        "        x1 = torch.flatten(input)[:128]\n",
        "        x2 = torch.flatten(input)[32768:(32768+128)]\n",
        "        x3 = torch.flatten(input)[65536:(65536+128)]\n",
        "        x4 = torch.flatten(input)[128:256]\n",
        "\n",
        "        out = torch.cat((x1, x2, x3, x4), 0)\n",
        "        out = torch.reshape(out, (1, self.num_clasess))\n",
        "        print(\"out shape\", out.shape)\n",
        "        return out\n",
        "\n",
        "model = BypassNet(num_clasess=512)\n",
        "to_onnx(model, input_shape, False)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "out shape torch.Size([1, 512])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:28: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n"
          ]
        }
      ]
    }
  ]
}